[{"content":"Recording https://www.bilibili.com/video/BV1P94y1a7Kf/?spm_id_from=333.999.0.0\u0026amp;vd_source=e9626f9767e6e22ece9d765f34ba01c5\nSpeaker Mark Ho\nBio Mark Ho is an Assistant Professor in the Computer Science Department at Stevens Institute of Technology. Previously, he was a faculty fellow at NYU and a postdoc at Princeton and UC Berkeley. He received his Ph.D in Cognitive Science and M.S. in Computer Science from Brown University. His research combines approaches from cognitive science, social psychology, and computer science to study the computational principles underlying human problem solving and social cognition.\nAbstract One of the most striking features of human intelligence is our capacity to rapidly and flexibly plan. Planning enables us to solve myriad everyday problems\u0026mdash;e.g., planning how to complete a list of errands on a busy day\u0026mdash;but planning is also very computationally demanding. How do we effectively plan despite fundamental constraints on our time, memory, and attention? My talk will cover recent work investigating how the flexible construction of task representations facilitates efficient planning in humans. I will discuss value-guided construal, a general formal theory of how people form simplified, ad hoc representations in order to plan and act. By investigating the computational principles that underlie how people construe problems, this approach provides a new perspective on the dynamic interplay of attention, causal reasoning, and goal-directed behavior.\n","permalink":"https://RLDMJC.github.io/posts/researchrecap/ho231004/","summary":"Recording https://www.bilibili.com/video/BV1P94y1a7Kf/?spm_id_from=333.999.0.0\u0026amp;vd_source=e9626f9767e6e22ece9d765f34ba01c5\nSpeaker Mark Ho\nBio Mark Ho is an Assistant Professor in the Computer Science Department at Stevens Institute of Technology. Previously, he was a faculty fellow at NYU and a postdoc at Princeton and UC Berkeley. He received his Ph.D in Cognitive Science and M.S. in Computer Science from Brown University. His research combines approaches from cognitive science, social psychology, and computer science to study the computational principles underlying human problem solving and social cognition.","title":"Research Recap | Construction of mental representations in human planning"},{"content":"Recording https://www.bilibili.com/video/BV1Rh4y1a7iq/?vd_source=e9626f9767e6e22ece9d765f34ba01c5\nSpeaker Haoxue Fan\nBio Haoxue Fan is a fifth-year graduate student at Harvard working with Dr. Elizabeth Phelps. She is interested in how emotion interacts with decision-making under uncertainty, including cognitive processes such as exploration, information seeking, and planning. To answer these questions, she uses a combination of computational modeling, physiological measurements, and behavioral experiments. She is originally from Shanghai in China, and is always eager to know where is the best coffee and boba tea place in town :)\nAbstract Exploration is at the core of many real-life decisions, helping people gain information about the environment and make better choices in the long run. Although anxiety has been related to decreased physical exploration and avoidance behavior, past findings on the interaction between anxiety and exploration during decision-making under uncertainty were inconclusive. The current study provides a holistic picture of the anxiety-exploration relationship by focusing on latent factors of trait anxiety and different exploration strategies when facing volatility-induced uncertainty. Across two well-powered online studies (N = 984), we demonstrated that people used a hybrid of directed, random, and undirected exploration strategies, which were respectively sensitive to relative uncertainty, total uncertainty, and value difference. The somatic factor of trait anxiety, the propensity to experience physical symptoms of anxiety, was inversely correlated with directed exploration and undirected exploration, manifesting as being less likely to choose the uncertain option and reducing choice stochasticity regardless of uncertainty. Trait somatic anxiety was also related to underestimation of relative uncertainty, which could potentially account for its negative impact on directed exploration. Together, these results reveal the selective role of trait somatic anxiety and physiological arousal in modulating uncertainty-driven and value-driven exploration strategies. If time permits, I will also present a follow-up study where we examined the relationship between transitory physiological arousal - indexed by real-time pupil size - and exploration strategies. We showed that pupil size positively correlates with total uncertainty, suggesting a selective role of physiological arousal in driving uncertainty-driven random exploration. Additionally, we were able to decode subject-specific uncertainty estimates from pupillary data, which improves the predictive power of participants’ trial-by-trial choice variability.\n","permalink":"https://RLDMJC.github.io/posts/researchrecap/fan230922/","summary":"Recording https://www.bilibili.com/video/BV1Rh4y1a7iq/?vd_source=e9626f9767e6e22ece9d765f34ba01c5\nSpeaker Haoxue Fan\nBio Haoxue Fan is a fifth-year graduate student at Harvard working with Dr. Elizabeth Phelps. She is interested in how emotion interacts with decision-making under uncertainty, including cognitive processes such as exploration, information seeking, and planning. To answer these questions, she uses a combination of computational modeling, physiological measurements, and behavioral experiments. She is originally from Shanghai in China, and is always eager to know where is the best coffee and boba tea place in town :)","title":"Research Recap | Trait somatic anxiety is associated with reduced directed exploration and underestimation of uncertainty"},{"content":"Recording https://www.bilibili.com/video/BV1sh4y1A7eu/\nSpeaker Stefano Palminteri\nBio I am a Research Director (equivalent of Full Professor) and head of the Human Reinforcement Learning team, which is part of the Laboratoire de Neurosciences Cognitives et Computationelles. My goal is understanding how humans learn to make decisions at the behavioral, computational and neural levels. I am mainly (but not only!) interested in situations when decisions are based on past experience (a.k.a. reinforcement learning). In the last few years I mainly worked on two computational hypotheses, \u0026ldquo;relative value\u0026rdquo; and \u0026ldquo;learning bias\u0026rdquo;, concerning human reinforcement learning. In addition to extending these two frameworks, new lines of research in my team investigate social learning , the experience/description gap and, more recently, the intersection between cognitive science and artificial intelligence. I also enjoy questioning the epistemological and methodological foundations of decision-making, neuroeconomics and cognitive science research.\nAbstract In the present talk I will review research performed by my and other laboratories that highlight the existence of reinforcement leaning biases. I will first start by briefly introducing the reinforcement learning framework and propose a taxonomy of biases within the framework itself. Then I will review direct empirical evidence supporting the existence of two learning biases: positivity bias and value-normalization at both the behavioral, neural and clinical level. Finally, I will present unpublished results of simulation studies that try to answer the question: what are these reinforcement learning biases good for?\n","permalink":"https://RLDMJC.github.io/posts/researchrecap/palminteri230915/","summary":"Recording https://www.bilibili.com/video/BV1sh4y1A7eu/\nSpeaker Stefano Palminteri\nBio I am a Research Director (equivalent of Full Professor) and head of the Human Reinforcement Learning team, which is part of the Laboratoire de Neurosciences Cognitives et Computationelles. My goal is understanding how humans learn to make decisions at the behavioral, computational and neural levels. I am mainly (but not only!) interested in situations when decisions are based on past experience (a.k.a. reinforcement learning). In the last few years I mainly worked on two computational hypotheses, \u0026ldquo;relative value\u0026rdquo; and \u0026ldquo;learning bias\u0026rdquo;, concerning human reinforcement learning.","title":"Research Recap | Reinforcement learning biases that makes us smart"},{"content":"Recording https://www.bilibili.com/video/BV1Sj411C77e/?vd_source=e9626f9767e6e22ece9d765f34ba01c5\nSpeaker Zhaoyu Zuo\nBio Zhaoyu Zuo obtained a master\u0026rsquo;s degree in pattern recognition and intelligent systems from the University of Science and Technology of China and is now a research assistant at Shanghai Jiao Tong University. He is concerned with the nature of human intelligence and the mechanisms that lead to behavior and mental disorders. He uses brain-inspired models to study the cognitive computational mechanisms behind complex decisions, particularly the cooperative relationship between memory systems and reinforcement learning.\nAbstract Model-based (goal-directed) decision-making requires prudent evaluation of the ultimate consequences of multi-stage choices. Previous studies have suggested that such evaluation relies on the reward experience accumulated by the reinforcement learning process in the brain. However, the core component of model-based decision-making\u0026ndash;working memory (WM), also retains reward information, and it is still unclear whether WM contributes to the evaluation. The current study analyzes four two-stage decision experiments, which separately manipulate two WM-related variables (delay and load). We found that time delay interfered with evaluating the ultimate consequences, while increased task load reduced cognitive effort in the feedback process and the probability of selecting the optimal option. Notably, our proposed models that corporated the reward-retained mechanism of WM could replicate the behavioral effects of delay and load, whereas the classical hybrid reinforcement learning model could not. Furthermore, individual-level analysis revealed a close correlation between model parameters and WM scores. Together, these results provide a deeper understanding of how reinforcement learning and WM co-work in complex decision-making and facilitate analysis of impaired model-based decision-making in clinical populations.\n","permalink":"https://RLDMJC.github.io/posts/researchrecap/zuo230909/","summary":"Recording https://www.bilibili.com/video/BV1Sj411C77e/?vd_source=e9626f9767e6e22ece9d765f34ba01c5\nSpeaker Zhaoyu Zuo\nBio Zhaoyu Zuo obtained a master\u0026rsquo;s degree in pattern recognition and intelligent systems from the University of Science and Technology of China and is now a research assistant at Shanghai Jiao Tong University. He is concerned with the nature of human intelligence and the mechanisms that lead to behavior and mental disorders. He uses brain-inspired models to study the cognitive computational mechanisms behind complex decisions, particularly the cooperative relationship between memory systems and reinforcement learning.","title":"Research Recap | Working memory guides action valuation in model-based inference"},{"content":"Recording https://www.bilibili.com/video/BV1qm4y1u7JU/?spm_id_from=333.999.0.0\u0026amp;vd_source=1a260a61416c0a766c7c16e727b2f404\nSpeaker Shen Xu\nBio Shen Xu is a third-year Ph.D. student from the School of Psychological and Cognitive Sciences at Peking University. He studies Basic Psychology under the supervision of Professor Dr. Xiaolin Zhou, and he is mainly keen on millisecond tactile information integration and computational social cognition. For the former, Shen focuses on how different tactile information is integrated and its temporal dynamics in the human brain. For the latter, he focuses on the computational and neural mechanisms of advantageous and disadvantageous unfair decision-making in Parkinson\u0026rsquo;s patients.\nAbstract In recent years, the use of high-precision computational models in social science, cognitive science, and affective neuroscience has increased dramatically. These models can be matched with empirical data to facilitate quantitative studies of the cognitive neural mechanisms underlying behavior. Yet most psychological researchers lack hands-on experience with computational modeling. In this tutorial, I will talk about decision-making tasks in social learning, and data-fitting using hierarchical Bayesian reinforcement learning (hBRL) models within a more comprehensive computational modeling framework. We collected behavioral data from 82 participants using the classic four-armed bandit task and explored the cognitive effects associated with each model parameter. Our tutorial provides more detailed theoretical and practical guidance to help novices to implement their own computational models and avoid common pitfalls.\n","permalink":"https://RLDMJC.github.io/posts/tutorialtalk/xu230826/","summary":"Recording https://www.bilibili.com/video/BV1qm4y1u7JU/?spm_id_from=333.999.0.0\u0026amp;vd_source=1a260a61416c0a766c7c16e727b2f404\nSpeaker Shen Xu\nBio Shen Xu is a third-year Ph.D. student from the School of Psychological and Cognitive Sciences at Peking University. He studies Basic Psychology under the supervision of Professor Dr. Xiaolin Zhou, and he is mainly keen on millisecond tactile information integration and computational social cognition. For the former, Shen focuses on how different tactile information is integrated and its temporal dynamics in the human brain. For the latter, he focuses on the computational and neural mechanisms of advantageous and disadvantageous unfair decision-making in Parkinson\u0026rsquo;s patients.","title":"Shen Xu | Crash tutorials on fitting hierarchical Bayesian reinforcement learning models"},{"content":" Title Speaker Time (UTC+8) Type Language Zoom (password) TBD Matt Nassar (Brown University) 2023/10/12 21:00 Research Recap English TBD TBD Seongmin Park (UC Davis) 2023/10/20 14:00 Research Recap English TBD TBD Taicheng Huang 黄泰诚 (Tsinghua University) 2023/10/23 TBD Journal Jam Chinese TBD TBD Charley Wu (University of Tübingen) 2023/11/08 16:00 Research Recap English TBD RLDDM Miqing Guo 郭鸣谦 (Radboud University) 2023/11/18 21：00 Tutorial Talk Chinese TBD TBD Weiji Ma 马伟基 (New York University) 2023/12/01 21：30 Research Recap English TBD Compositionality (Exact title TBD) Michael J Frank (Brown University) 2023/12/14 21：00 Research Recap English TBD ","permalink":"https://RLDMJC.github.io/upcoming/","summary":" Title Speaker Time (UTC+8) Type Language Zoom (password) TBD Matt Nassar (Brown University) 2023/10/12 21:00 Research Recap English TBD TBD Seongmin Park (UC Davis) 2023/10/20 14:00 Research Recap English TBD TBD Taicheng Huang 黄泰诚 (Tsinghua University) 2023/10/23 TBD Journal Jam Chinese TBD TBD Charley Wu (University of Tübingen) 2023/11/08 16:00 Research Recap English TBD RLDDM Miqing Guo 郭鸣谦 (Radboud University) 2023/11/18 21：00 Tutorial Talk Chinese TBD TBD Weiji Ma 马伟基 (New York University) 2023/12/01 21：30 Research Recap English TBD Compositionality (Exact title TBD) Michael J Frank (Brown University) 2023/12/14 21：00 Research Recap English TBD ","title":"Upcoming Events"},{"content":"Recording https://www.bilibili.com/video/BV1au4y1R7kh/?spm_id_from=333.999.0.0\u0026amp;vd_source=e9626f9767e6e22ece9d765f34ba01c5\nSpeaker Huadong Xiong\nBio I am a first-year Ph.D student in the CNS program, the department of psychology at the University of Arizona, directed by Dr. Robert Wilson. I build models to understand behaviors and I study how computation could be implemented in neural networks. Following the release of GPT-4, my research interest has partially shifted towards understanding the emergence of intelligence within large language models.\nAbstract The exploration-exploitation trade-off, balancing the acquisition of new information with the utilization of known resources, is a fundamental dilemma faced by all adaptive intelligence. Despite our understanding of models based on normative principles, the diverse explore-exploit behaviors of natural intelligence remain elusive. Here, using neural network behavioral modeling and state space analysis, we examined the diverse human exploration behaviors under a novel two-armed bandit task, designed to simulate real-world environmental volatility where exploration becomes essential. Examining behavior in the belief state space of this task, we characterized the disparities across artificial agents with decision boundaries. To extend this analysis to human data, a circumstance where choices are too sparse in the belief state space, we trained a recurrent neural network (RNN) model to predict humans’ choices given past observations. This RNN model outperforms all existing cognitive models. Probing the RNN’s decision boundaries, we found substantial individual differences that evade classical cognitive models. Additionally, our RNN revealed a model-based pattern employed by humans in response to higher environmental volatilities. Our work offers a promising approach for investigating diverse decision-making strategies in humans and animals.\n","permalink":"https://RLDMJC.github.io/posts/researchrecap/xiong230809/","summary":"Recording https://www.bilibili.com/video/BV1au4y1R7kh/?spm_id_from=333.999.0.0\u0026amp;vd_source=e9626f9767e6e22ece9d765f34ba01c5\nSpeaker Huadong Xiong\nBio I am a first-year Ph.D student in the CNS program, the department of psychology at the University of Arizona, directed by Dr. Robert Wilson. I build models to understand behaviors and I study how computation could be implemented in neural networks. Following the release of GPT-4, my research interest has partially shifted towards understanding the emergence of intelligence within large language models.\nAbstract The exploration-exploitation trade-off, balancing the acquisition of new information with the utilization of known resources, is a fundamental dilemma faced by all adaptive intelligence.","title":"Research Recap | Neural network modeling reveals diverse human exploration behaviors via state space analysis"},{"content":"MindRL is an academic commuity for researchers in the field of reinforcement learning. We co-founded this community with the hope of providing a platform for researchers to communicate and collaborate. We welcome peers from fields such as psychology, cognitive neuroscience, artificial intelligence, and other related disciplines such as mathematics, management and economics to build a more diverse and inclusive platform.\nWe have various activities such as:\nResearch Recap: researchers share their original research in RL. Journal Jam: community members interpret and report on journal articles in RL. Book Breakdown: community members read classic and cutting-edge books together. Tutorial Talks: community members share tutorials and practical experiences in RL. Tool Time: community members share the use and development of tools in RL. Who are we At the heart of the MindRL Hub Community lies the collective vision of four passionate friends, united by a common purpose to foster innovation, knowledge sharing, and collaboration in the fields of psychology, neuroscience and reinforcement learning.\nOur organizing committee is composed of Zeming Fang, Huaiyu Liu, Yanan Liu, and Hanbo Xie (in order of surname), each bringing a unique blend of expertise, experience, and cultural perspectives to the table. Hailing from diverse corners of the world, based on China, the UK, Canada, and the United States, we proudly represent a global network of minds dedicated to advancing the frontiers of reinforcement learning.\nWelcome to a community where geographical boundaries blur, and ideas flow freely across borders. How to join us If you want to join the community or participate in an event, please feel free to contact us via email (rldmjc2023@gmail.com).\n","permalink":"https://RLDMJC.github.io/about/","summary":"about","title":"What is MindRL"}]