[{"content":"Recording https://www.bilibili.com/video/BV1Sj411C77e/?vd_source=e9626f9767e6e22ece9d765f34ba01c5\nSpeaker Zhaoyu Zuo\nBio Zhaoyu Zuo obtained a master\u0026rsquo;s degree in pattern recognition and intelligent systems from the University of Science and Technology of China and is now a research assistant at Shanghai Jiao Tong University. He is concerned with the nature of human intelligence and the mechanisms that lead to behavior and mental disorders. He uses brain-inspired models to study the cognitive computational mechanisms behind complex decisions, particularly the cooperative relationship between memory systems and reinforcement learning.\nAbstract Model-based (goal-directed) decision-making requires prudent evaluation of the ultimate consequences of multi-stage choices. Previous studies have suggested that such evaluation relies on the reward experience accumulated by the reinforcement learning process in the brain. However, the core component of model-based decision-making\u0026ndash;working memory (WM), also retains reward information, and it is still unclear whether WM contributes to the evaluation. The current study analyzes four two-stage decision experiments, which separately manipulate two WM-related variables (delay and load). We found that time delay interfered with evaluating the ultimate consequences, while increased task load reduced cognitive effort in the feedback process and the probability of selecting the optimal option. Notably, our proposed models that corporated the reward-retained mechanism of WM could replicate the behavioral effects of delay and load, whereas the classical hybrid reinforcement learning model could not. Furthermore, individual-level analysis revealed a close correlation between model parameters and WM scores. Together, these results provide a deeper understanding of how reinforcement learning and WM co-work in complex decision-making and facilitate analysis of impaired model-based decision-making in clinical populations.\n","permalink":"https://RLDMJC.github.io/posts/researchrecap/zuo230909/","summary":"Recording https://www.bilibili.com/video/BV1Sj411C77e/?vd_source=e9626f9767e6e22ece9d765f34ba01c5\nSpeaker Zhaoyu Zuo\nBio Zhaoyu Zuo obtained a master\u0026rsquo;s degree in pattern recognition and intelligent systems from the University of Science and Technology of China and is now a research assistant at Shanghai Jiao Tong University. He is concerned with the nature of human intelligence and the mechanisms that lead to behavior and mental disorders. He uses brain-inspired models to study the cognitive computational mechanisms behind complex decisions, particularly the cooperative relationship between memory systems and reinforcement learning.","title":"Research Recap | Working memory guides action valuation in model-based inference"},{"content":"Recording https://www.bilibili.com/video/BV1qm4y1u7JU/?spm_id_from=333.999.0.0\u0026amp;vd_source=1a260a61416c0a766c7c16e727b2f404\nSpeaker Shen Xu\nBio Shen Xu is a third-year Ph.D. student from the School of Psychological and Cognitive Sciences at Peking University. He studies Basic Psychology under the supervision of Professor Dr. Xiaolin Zhou, and he is mainly keen on millisecond tactile information integration and computational social cognition. For the former, Shen focuses on how different tactile information is integrated and its temporal dynamics in the human brain. For the latter, he focuses on the computational and neural mechanisms of advantageous and disadvantageous unfair decision-making in Parkinson\u0026rsquo;s patients.\nAbstract In recent years, the use of high-precision computational models in social science, cognitive science, and affective neuroscience has increased dramatically. These models can be matched with empirical data to facilitate quantitative studies of the cognitive neural mechanisms underlying behavior. Yet most psychological researchers lack hands-on experience with computational modeling. In this tutorial, I will talk about decision-making tasks in social learning, and data-fitting using hierarchical Bayesian reinforcement learning (hBRL) models within a more comprehensive computational modeling framework. We collected behavioral data from 82 participants using the classic four-armed bandit task and explored the cognitive effects associated with each model parameter. Our tutorial provides more detailed theoretical and practical guidance to help novices to implement their own computational models and avoid common pitfalls.\n","permalink":"https://RLDMJC.github.io/posts/tutorialtalk/xu230826/","summary":"Recording https://www.bilibili.com/video/BV1qm4y1u7JU/?spm_id_from=333.999.0.0\u0026amp;vd_source=1a260a61416c0a766c7c16e727b2f404\nSpeaker Shen Xu\nBio Shen Xu is a third-year Ph.D. student from the School of Psychological and Cognitive Sciences at Peking University. He studies Basic Psychology under the supervision of Professor Dr. Xiaolin Zhou, and he is mainly keen on millisecond tactile information integration and computational social cognition. For the former, Shen focuses on how different tactile information is integrated and its temporal dynamics in the human brain. For the latter, he focuses on the computational and neural mechanisms of advantageous and disadvantageous unfair decision-making in Parkinson\u0026rsquo;s patients.","title":"Tutorial Talk | Crash tutorials on fitting hierarchical Bayesian reinforcement learning models"},{"content":" Title Speaker Time (UTC+8) Category TBD Stefano Palminteri (INSERM) 2023/09/15 TBD Research Recap TBD Mark Ho (Stevens Institute) 2023/10/04 21:00 Research Recap TBD Matt Nassar (Brown University) 2023/10/12 21:00 Research Recap TBD Seongmin Park (UC Davis) 2023/10/20 14:00 Research Recap TBD Taicheng Huang 黄泰诚 (Tsinghua University) 2023/10/23 TBD Journal Jam TBD Charley Wu (University of Tübingen) 2023/11/08 16:00 Research Recap RLDDM Miqing Guo 郭鸣谦 (Radboud University) 2023/11/18 21：00 Tutorial Talk TBD Weiji Ma 马伟基 (New York University) 2023/12/01 21：30 Research Recap Compositionality (Exact title TBD) Michael J Frank (Brown University) 2023/12/14 21：00 Research Recap ","permalink":"https://RLDMJC.github.io/upcoming/","summary":" Title Speaker Time (UTC+8) Category TBD Stefano Palminteri (INSERM) 2023/09/15 TBD Research Recap TBD Mark Ho (Stevens Institute) 2023/10/04 21:00 Research Recap TBD Matt Nassar (Brown University) 2023/10/12 21:00 Research Recap TBD Seongmin Park (UC Davis) 2023/10/20 14:00 Research Recap TBD Taicheng Huang 黄泰诚 (Tsinghua University) 2023/10/23 TBD Journal Jam TBD Charley Wu (University of Tübingen) 2023/11/08 16:00 Research Recap RLDDM Miqing Guo 郭鸣谦 (Radboud University) 2023/11/18 21：00 Tutorial Talk TBD Weiji Ma 马伟基 (New York University) 2023/12/01 21：30 Research Recap Compositionality (Exact title TBD) Michael J Frank (Brown University) 2023/12/14 21：00 Research Recap ","title":"Upcoming Events"},{"content":"Recording https://www.bilibili.com/video/BV1au4y1R7kh/?spm_id_from=333.999.0.0\u0026amp;vd_source=e9626f9767e6e22ece9d765f34ba01c5\nSpeaker Huadong Xiong\nBio I am a first-year Ph.D student in the CNS program, the department of psychology at the University of Arizona, directed by Dr. Robert Wilson. I build models to understand behaviors and I study how computation could be implemented in neural networks. Following the release of GPT-4, my research interest has partially shifted towards understanding the emergence of intelligence within large language models.\nAbstract The exploration-exploitation trade-off, balancing the acquisition of new information with the utilization of known resources, is a fundamental dilemma faced by all adaptive intelligence. Despite our understanding of models based on normative principles, the diverse explore-exploit behaviors of natural intelligence remain elusive. Here, using neural network behavioral modeling and state space analysis, we examined the diverse human exploration behaviors under a novel two-armed bandit task, designed to simulate real-world environmental volatility where exploration becomes essential. Examining behavior in the belief state space of this task, we characterized the disparities across artificial agents with decision boundaries. To extend this analysis to human data, a circumstance where choices are too sparse in the belief state space, we trained a recurrent neural network (RNN) model to predict humans’ choices given past observations. This RNN model outperforms all existing cognitive models. Probing the RNN’s decision boundaries, we found substantial individual differences that evade classical cognitive models. Additionally, our RNN revealed a model-based pattern employed by humans in response to higher environmental volatilities. Our work offers a promising approach for investigating diverse decision-making strategies in humans and animals.\n","permalink":"https://RLDMJC.github.io/posts/researchrecap/xiong230809/","summary":"Recording https://www.bilibili.com/video/BV1au4y1R7kh/?spm_id_from=333.999.0.0\u0026amp;vd_source=e9626f9767e6e22ece9d765f34ba01c5\nSpeaker Huadong Xiong\nBio I am a first-year Ph.D student in the CNS program, the department of psychology at the University of Arizona, directed by Dr. Robert Wilson. I build models to understand behaviors and I study how computation could be implemented in neural networks. Following the release of GPT-4, my research interest has partially shifted towards understanding the emergence of intelligence within large language models.\nAbstract The exploration-exploitation trade-off, balancing the acquisition of new information with the utilization of known resources, is a fundamental dilemma faced by all adaptive intelligence.","title":"Research Recap | Neural network modeling reveals diverse human exploration behaviors via state space analysis"},{"content":"MindRL is an academic commuity for researchers in the field of reinforcement learning. We co-founded this community with the hope of providing a platform for researchers to communicate and collaborate. We welcome peers from fields such as psychology, cognitive neuroscience, artificial intelligence, and other related disciplines such as mathematics, management and economics to build a more diverse and inclusive platform.\nWe have various activities such as:\nResearch Recap: researchers share their original research in RL. Journal Jam: community members interpret and report on journal articles in RL. Book Breakdown: community members read classic and cutting-edge books together. Tutorial Talks: community members share tutorials and practical experiences in RL. Tool Time: community members share the use and development of tools in RL. Who are we There are four members in our organizing committee as listed below (in the order of surname): How to join us If you want to join the community or participate in an event, please feel free to contact us via email (rldmjc2023@gmail.com).\n","permalink":"https://RLDMJC.github.io/about/","summary":"about","title":"What is MindRL"}]